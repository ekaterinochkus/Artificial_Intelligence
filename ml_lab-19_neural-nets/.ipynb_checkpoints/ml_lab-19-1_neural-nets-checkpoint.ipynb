{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторна робота 19-1. Реализация нейронной сети своими руками.\n",
    "\n",
    "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
    "\n",
    "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
    "\n",
    "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
    "from layers import FullyConnectedLayer, ReLULayer\n",
    "from model import TwoLayerNet\n",
    "from trainer import Trainer, Dataset\n",
    "from optim import SGD, MomentumSGD\n",
    "from metrics import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "\n",
    "И разделяем их на training и validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp/ipykernel_13988/2810386256.py:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
      "C:\\Users\\User\\AppData\\Local\\Temp/ipykernel_13988/2810386256.py:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n"
     ]
    }
   ],
   "source": [
    "def prepare_for_neural_network(train_X, test_X):\n",
    "#   reshape() изменяет форму массива без изменения его данных\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    return train_flat, test_flat\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9000, 3072), (1000, 3072))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, начинаем с кирпичиков\n",
    "\n",
    "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
    "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
    "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
    "\n",
    "Начнем с ReLU, у которого параметров нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Not implemented!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13988/2690710071.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m               ])\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0mcheck_layer_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mReLULayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AI\\Labs\\ml_lab-19_neural-nets\\gradient_check.py\u001b[0m in \u001b[0;36mcheck_layer_gradient\u001b[1;34m(layer, x, delta, tol)\u001b[0m\n\u001b[0;32m    189\u001b[0m       \u001b[0mbool\u001b[0m \u001b[0mindicating\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0mgradients\u001b[0m \u001b[0mmatch\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m     \"\"\"\n\u001b[1;32m--> 191\u001b[1;33m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m     \u001b[0moutput_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AI\\Labs\\ml_lab-19_neural-nets\\layers.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;31m# Hint: you'll need to save some information about X\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;31m# to use it later in the backward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Not implemented!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Not implemented!"
     ]
    }
   ],
   "source": [
    "# TODO: Implement ReLULayer layer in layers.py\n",
    "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
    "\n",
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "\n",
    "assert check_layer_gradient(ReLULayer(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
    "\n",
    "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
    "\n",
    "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Not implemented!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13988/485996562.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# TODO: Implement FullyConnected layer forward and backward methods\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0mcheck_layer_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFullyConnectedLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# TODO: Implement storing gradients for W and B\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# your code here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AI\\Labs\\ml_lab-19_neural-nets\\gradient_check.py\u001b[0m in \u001b[0;36mcheck_layer_gradient\u001b[1;34m(layer, x, delta, tol)\u001b[0m\n\u001b[0;32m     57\u001b[0m       \u001b[0mbool\u001b[0m \u001b[0mindicating\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0mgradients\u001b[0m \u001b[0mmatch\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \"\"\"\n\u001b[1;32m---> 59\u001b[1;33m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m     \u001b[0moutput_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AI\\Labs\\ml_lab-19_neural-nets\\layers.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[1;31m# TODO: Implement forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;31m# Your final implementation shouldn't have any loops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Not implemented!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Not implemented!"
     ]
    }
   ],
   "source": [
    "# TODO: Implement FullyConnected layer forward and backward methods\n",
    "# Кросс-энтропияизмеряет расхождение между двумя вероятностными распределениями \n",
    "# Если кросс-энтропия большая - разница между двумя распределениями велика, если небольшая - распределения похожи друг на друга.\n",
    "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
    "# TODO: Implement storing gradients for W and B\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем нейронную сеть\n",
    "\n",
    "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
    "\n",
    "Не забудьте реализовать очистку градиентов в начале функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Not implemented!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13988/938906758.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# TODO: In model.py, implement compute_loss_and_gradients function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTwoLayerNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_layer_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss_and_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# TODO Now implement backward pass and aggregate all of the params\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AI\\Labs\\ml_lab-19_neural-nets\\model.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, n_input, n_output, hidden_layer_size, reg)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m# TODO Create necessary layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Not implemented!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcompute_loss_and_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Not implemented!"
     ]
    }
   ],
   "source": [
    "# TODO: In model.py, implement compute_loss_and_gradients function\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
    "loss = model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "\n",
    "# TODO Now implement backward pass and aggregate all of the params\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Not implemented!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13988/3421380310.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# TODO Now implement l2 regularization in the forward and backward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel_with_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTwoLayerNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_layer_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mloss_with_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_with_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss_and_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mloss_with_reg\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_with_reg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;34m\"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_with_reg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AI\\Labs\\ml_lab-19_neural-nets\\model.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, n_input, n_output, hidden_layer_size, reg)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m# TODO Create necessary layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Not implemented!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcompute_loss_and_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Not implemented!"
     ]
    }
   ],
   "source": [
    "# TODO Now implement l2 regularization in the forward and backward pass\n",
    "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
    "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
    "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
    "\n",
    "check_model_gradient(model_with_reg, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
    "\n",
    "Какое значение точности мы ожидаем увидеть до начала тренировки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, implement predict function!\n",
    "\n",
    "# TODO: Implement predict function\n",
    "# What would be the value we expect?\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Допишем код для процесса тренировки\n",
    "\n",
    "Если все реализовано корректно, значение функции ошибки должно уменьшаться с каждой эпохой, пусть и медленно. Не беспокойтесь пока про validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Not implemented!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13988/688738125.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTwoLayerNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_layer_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e-2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# TODO Implement missing pieces in Trainer.fit function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AI\\Labs\\ml_lab-19_neural-nets\\model.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, n_input, n_output, hidden_layer_size, reg)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m# TODO Create necessary layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Not implemented!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcompute_loss_and_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Not implemented!"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate = 1e-2)\n",
    "\n",
    "# TODO Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down every epoch, even if it's slow\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13988/1549702324.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_history\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_history\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_history' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшаем процесс тренировки\n",
    "\n",
    "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уменьшение скорости обучения (learning rate decay)\n",
    "\n",
    "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
    "\n",
    "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
    "\n",
    "В нашем случае N будет равным 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_13988/2597873135.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\User\\AppData\\Local\\Temp/ipykernel_13988/2597873135.py\"\u001b[1;36m, line \u001b[1;32m8\u001b[0m\n\u001b[1;33m    initial_learning_rate = # your code here\u001b[0m\n\u001b[1;37m                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# TODO Implement learning rate decay inside Trainer.fit method\n",
    "# Decay should happen once per epoch\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate_decay=0.99)\n",
    "\n",
    "initial_learning_rate = # your code here\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
    "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Накопление импульса (Momentum SGD)\n",
    "\n",
    "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
    "\n",
    "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
    "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
    "\n",
    "```\n",
    "velocity = momentum * velocity - learning_rate * gradient \n",
    "w = w + velocity\n",
    "```\n",
    "\n",
    "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
    "\n",
    "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
    "http://cs231n.github.io/neural-networks-3/#sgd  \n",
    "https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Not implemented!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13988/2001557021.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# TODO: Implement MomentumSGD.update function in optim.py\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTwoLayerNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_layer_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e-1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMomentumSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.99\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AI\\Labs\\ml_lab-19_neural-nets\\model.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, n_input, n_output, hidden_layer_size, reg)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m# TODO Create necessary layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Not implemented!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcompute_loss_and_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Not implemented!"
     ]
    }
   ],
   "source": [
    "# TODO: Implement MomentumSGD.update function in optim.py\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-4, learning_rate_decay=0.99)\n",
    "\n",
    "# You should see even better results than before!\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну что, давайте уже тренировать сеть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
    "\n",
    "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
    "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
    "\n",
    "Если этого не происходит, то где-то была допущена ошибка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Not implemented!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13988/631038126.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdata_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTwoLayerNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_layer_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e-1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdata_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdata_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_X\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdata_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdata_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AI\\Labs\\ml_lab-19_neural-nets\\model.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, n_input, n_output, hidden_layer_size, reg)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m# TODO Create necessary layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Not implemented!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcompute_loss_and_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Not implemented!"
     ]
    }
   ],
   "source": [
    "data_size = 15\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=150, batch_size=5)\n",
    "\n",
    "# You should expect this to reach 1.0 training accuracy \n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
    "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
    "Найдите их!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Not implemented!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13988/3632182432.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTwoLayerNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_layer_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e-1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdata_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdata_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_X\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdata_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdata_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AI\\Labs\\ml_lab-19_neural-nets\\model.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, n_input, n_output, hidden_layer_size, reg)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m# TODO Create necessary layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Not implemented!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcompute_loss_and_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Not implemented!"
     ]
    }
   ],
   "source": [
    "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=20, batch_size=5)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итак, основное мероприятие!\n",
    "\n",
    "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
    "\n",
    "Добейтесь точности лучше **60%** на validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "must be real number, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13988/1421034157.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# your code here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best validation accuracy achieved: %f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbest_val_accuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: must be real number, not NoneType"
     ]
    }
   ],
   "source": [
    "# Let's train the best one-hidden-layer network we can\n",
    "\n",
    "learning_rates = 1e-4\n",
    "reg_strength = 1e-3\n",
    "learning_rate_decay = 0.999\n",
    "hidden_layer_size = 128\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "loss_history = []\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "# TODO find the best hyperparameters to train the network\n",
    "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
    "# You should expect to get to at least 40% of valudation accuracy\n",
    "# Save loss/train/history of the best classifier to the variables above\n",
    "# your code here\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1fa3dd7ae80>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(211)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss_history)\n",
    "plt.subplot(212)\n",
    "plt.title(\"Train/validation accuracy\")\n",
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13988/323908355.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbest_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmulticlass_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Neural net test set accuracy: %f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtest_accuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
